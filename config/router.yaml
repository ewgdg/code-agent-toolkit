# Claude Code Model Router Configuration
# This configuration routes Claude Code CLI traffic to appropriate endpoints

router:
  listen: "0.0.0.0:8787"
  original_base_url: "https://api.anthropic.com"

# Provider configurations
providers:
  openai:
    base_url: "https://api.openai.com/v1"
    adapter: "openai"
    api_key_env: "OPENAI_API_KEY"
    tools:
      restricted_tool_names:
        - WebFetch
        - WebSearch
  llama:
    base_url: "http://127.0.0.1:8080/v1"
    adapter: "openai-compatible"
    tools:
      restricted_tool_names:
        - WebFetch
        - WebSearch

# OpenAI configuration
openai:
  reasoning_effort_default: "minimal"  # Cost-efficient default
  reasoning_thresholds:
    low_max: 2048      # 1-5K tokens: simple tasks (~20% of max_tokens)
    medium_max: 8192  # 5K-15K tokens: balanced tasks (~50% of max_tokens)
    # 15K+ tokens: high effort complex reasoning (up to 32K cap)
  reasoning_model_prefixes:
    - "o"
    - "gpt-5"
    - "gpt-oss"

# Timeout configuration
timeouts_ms:
  connect: 10000    # 10 seconds
  read: 300000

# Logging configuration
logging:
  level: "info"

# Override rules for routing decisions
overrides:
  # Route plan mode requests to reasoning model
  - when:
      request:
        user_regex: '<system-reminder>[\s\S]*\b(?:plan mode is (?:activated|active|triggered|on)|in plan mode)\b[\s\S]*</system-reminder>'
    provider: "openai"
    model: "gpt-5"
    support_reasoning: true
    config:
      reasoning:
        effort: 
          value: "medium"
          when:
            current_in: [null, "low", "minimal"]  # Apply only if unset, low, or minimal
        summary: "auto"
  
  # Passthrough sonnet model request
  - when:
      request:
        model_regex: 'sonnet'
    provider: "anthropic"
      
  - when:
      request:
        model_regex: 'haiku'
    provider: "anthropic"

  # Route opus model requests to powerful OpenAI model
  - when:
      request:
        model_regex: 'opus'
    provider: "openai"
    model: "gpt-5"
    
  # Route any model containing "gpt-mini" to background model
  - when:
      request:
        model_regex: '^gpt-mini$'
    provider: "openai"
    model: "gpt-5-mini"
  
  # Route specific model with reasoning configuration
  - when:
      request:
        model_regex: '^gpt-high$'
    model: "gpt-5"
    provider: "openai"
    config:
      reasoning:
        effort:
          value: "high"
        summary: "auto"
  
  # Route gpt requests to OpenAI
  - when:
      request:
        model_regex: '^gpt$'
    provider: "openai"
    model: "gpt-5"

  - when:
      request:
        model_regex: '^gpt-[0-9]+.*'
    provider: "openai"

  - when:
      request:
        model_regex: '^o[0-9]+(?:-.*)?'
    provider: "openai"
  
  - when:
      request:
        model_regex: '.*'
    provider: "llama"
